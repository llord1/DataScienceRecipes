## Metalinguistic Analysis

### Procedure

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.
At vero eos et accusam et justo duo dolores et ea rebum.
Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.
Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.
At vero eos et accusam et justo duo dolores et ea rebum.
Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.
Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.
At vero eos et accusam et justo duo dolores et ea rebum.
Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

After the corpus was collected and converted to plain text, it was tokenized using the NLTK's ^[http://www.nltk.org] implementation of the Punkt sentence tokenizer [@kiss2006unsupervised] followed by the Penn Treebank word tokenizer [@marcus1993building].
Punkt was chosen as it has proven viable in several other works [@hiippala2016semi;@avramidis2011evaluate;@yao2014information;@nothman2013learning;@marrese2014novel].
Laboreiro et al. notes that while the Penn Treebank may have some issues, it is still the de-facto standard [@laboreiro2010tokenizing].

### Linguistic variables analyzed

This paper investigated several dependent variables.
Overall document cadence was captured using mean words per sentence, mean sentences per paragraph, and mean paragraphs per document.
Lexical diversity was captured using a moving average type-token ratio (MATTR) [@covington2010cutting] with a window size 1000.

#### Basic

What collection of characters are recognized as distinct from one another may differ depending on the research at hand.
In order to maximize reproducibility, this paper adopts the definitions of words and sentences presented by tokenizers listed above [@kiss2006unsupervised;@marrese2014novel].
Two neighboring sentences are said to come from distinct paragraphs if they are separated by at least one blank line.

#### Type-token ratio

A type-token ratio is simple to compute measure of word diversity inside of a single document [@hill2015real].
However, it suffers from the need to standardize on document length [@covington2010cutting;@kettunen2014can;@jarvis2002short;@richards1987type].
A moving average type-token ratio helps address these issues [@covington2010cutting;@kettunen2014can].

#### Statistical analyses

The measures were extracted from the corpus then analysised using a multivariate analysis of variance (MANOVA) approach.
The total number of documents in the corpus was N = `r dim(meta$metadata)[1]`.
$\alpha$ and $\beta$ were left at their traditional levels.

