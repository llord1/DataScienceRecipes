---
title: 'Metalinguistic Analysis'
author: "Mark Newman"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, echo = F, results = 'hide', error = F, warning = F, message = F}
rm(list = ls())
options(scipen = 999)

libraries <- c('dplyr', 'tidyr', 'ggpubr', 'mnmacros', 'nortest', 'mvnormtest', 'biotools', 'psych', 'car')
# if needed
# install.packages('devtools')
# devtools::install_github("markanewman/mnmacros")
# install.packages(libraries)
x = lapply(libraries, library, character.only = TRUE)
```

A Metalinguistic Analysis compares the linguistic features of a corpus across some metadata segment.
The features are extracted on a document by document basis.
Then a MANOVA is preformed comparing the segments.

In order to perform a Metalinguistic Analysis, documents must first be obtained.
This can be done by any means.
Following data collection, the documents need to be processed and the features extracted.
R is _OK_ enough at this task, but as the document count increases, R _really_ slows down.
Because of this, the feature extraction step is preformed in Pure Python.
The walk-through of this can be found in `../code/01-MetalinguisticAnalysisPipeline.md`

# Data format
 
For this recipe, 3 files are expected:

* A `metadata.csv` file containing metadata data about the individual documents.
  This file needs to include at _least_ 2 columns:
  1. **id**: the document's id
  2. the segment
* A `cadence.csv` file containing frequency count measures.
  This file will contain 4 columns:
  1. **id**: the document's id
  2. **Words**: Total words in the document
  3. **Sentences**: Total sentences in the document
  4. **Paragraphs**: Total paragraphs in the document
* A `diversity.csv` file containing diversity measures.
  This file will contain 4 columns:
  1. **id**: the document's id
  2. **TTR**: the type-token ratio (TTR)
  3. **MATTR500**: the moving average type-token ratio (MATTR)
     * Window length 500
  4. **MATTR1k**: the moving average type-token ratio
     * Window length 1000

```{r, results = 'hold'}
metadata <- read.csv('../data/metadata.csv')
cadence <- read.csv('../data/cadence.csv')
diversity <- read.csv('../data/diversity.csv')
```

# Check Assumptions.

Normally, MANOVA is known to be robust.
What this _actualy_ means is to works well if the assumptions are violated, but only under certain circumstances.
The circumstance being a balanced design.
Unfortunately, a metalinguistic analysis is almost never balanced.
This means we are at least forced to use type III SS measures

The assumptions we need to test are as follows:

* Independent Observations
* Normality
* Equal variance-covariance matrix

## Charts

Looking at charts helps inform us about our data.
They are not a _test_ of anything, but they can help us make decisions out of the gate.

```{r}
cadence_words <-
  metadata %>%
  merge(cadence) %>%
  transmute(author, Words) %>%
  gather(type, count, -author) %>%
  mutate(type = as.factor(type))

word_density_plot <-
  ggdensity(cadence_words,
    x = 'count',
    add = 'mean',
    color = 'type',
    palette = get_palette('Dark2', 1),
    title = 'Word Distribution')

cadence_sentence <-
  metadata %>%
  merge(cadence) %>%
  transmute(author, Sentences) %>%
  gather(type, count, -author) %>%
  mutate(type = as.factor(type))

sentence_density_plot <-
  ggdensity(cadence_sentence,
    x = 'count',
    add = 'mean',
    color = 'type',
    palette = get_palette('Dark2', 1),
    title = 'Sentence Distribution')

cadence_paragraph <-
  metadata %>%
  merge(cadence) %>%
  transmute(author, Paragraphs) %>%
  gather(type, count, -author) %>%
  mutate(type = as.factor(type))

paragraph_density_plot <-
  ggdensity(cadence_paragraph,
    x = 'count',
    add = 'mean',
    color = 'type',
    palette = get_palette('Dark2', 1),
    title = 'Paragraph Distribution')

diversity2 <-
  metadata %>%
  merge(diversity) %>%
  transmute(author, TTR, MATTR500, MATTR1k) %>%
  gather(type, count, -author) %>%
  mutate(type = as.factor(type))

diversity_density_plot <-
  ggdensity(diversity2,
    x = 'count',
    add = 'mean',
    color = 'type',
    palette = get_palette('Dark2', 3),
    title = 'TTR / MATTR Distribution')

```

```{r word-density}
plot(word_density_plot)
```

```{r sentence-density}
plot(sentence_density_plot)
```

```{r paragraph-density}
plot(paragraph_density_plot)
```

```{r diversity-density}
plot(diversity_density_plot)
```

All the figures look like they _might_ already be normal, or that at least we can wrangle them into being normal.

## Independent Observations

Independent Observations here is not that we included 2 documents twice, but that the measure is measuring the same thing.

```{r, results = 'hold'}
(icc_cadence <- ICC(cadence[, !names(cadence) %in% c('id')]))
```

Looking at the 1st ICC, we can see a non-significant (p = `r round(icc_cadence$results$p[1], 3)`) low correlation (ICC = `r round(icc_cadence$results$ICC[1], 3)`).
The omnibus test fails to reject the null hypothesis and so we conclude that there is not sufficient evidence to support the different measures being dependent.
_If_ the measures _were_ dependent we loose the rational for running a MANOVA.
A simple ANOVA on the strongest measure (usually words) would be the next step

```{r, results = 'hold'}
(icc_diversity <- ICC(diversity[, !names(cadence) %in% c('id')]))
```

Looking at the 1st ICC, we can see a non-significant (p = `r round(icc_diversity$results$p[1], 3)`) low correlation (ICC = `r round(icc_diversity$results$ICC[1], 3)`).
The omnibus test fails to reject the null hypothesis and so we conclude that there is not sufficient evidence to support the different measures being dependent.
However, we know our data.
The MATTR is essentially trying to get a better estimate of the true document normal diversity when compared to the normal TTR.
So it makes sense to check some pair wise values too.

```{r, results = 'hold'}
(ct_1 <- cor.test(diversity[, 'TTR'], diversity[, 'MATTR500']))
(ct_2 <- cor.test(diversity[, 'TTR'], diversity[, 'MATTR1k']))
(ct_3 <- cor.test(diversity[, 'MATTR500'], diversity[, 'MATTR1k']))
```

When comparing TTR to either MATTR (either 500 or 1000), we find a significant ($p$ = `r round(ct_1$p.value, 3)` / $p$ = `r round(ct_2$p.value, 3)`) medium correlation ($r$ = `r round(unname(ct_1$estimate), 3)` / $r$ = `r round(unname(ct_2$estimate), 3)`).
In both cases, the pair-wise test rejects the null hypothesis and so we concluder that there is evidence to support the different measures being dependent.
When comparing the two different window lengths of MATTR, we find a significant ($p$ = `r round(ct_3$p.value, 3)`) hi correlation ($r$ = `r round(unname(ct_3$estimate), 3)`).
The pair-wise test rejects the null hypothesis and so we concluder that there is evidence to support the different measures being dependent.
Based on the above evidence, we elect to proceed with just the MATTR1k.

Gather up all the dependent variables at interest into a single frame.
Preform one last omnibus ICC to make sure we are still good.

```{r, results = 'hold'}
data <-
  metadata %>%
  merge(cadence) %>%
  merge(diversity) %>%
  transmute(author, Words, Sentences, Paragraphs, MATTR1k)

(icc_data = ICC(data[, !names(data) %in% c('author')]))
```

Looking at the 1st ICC, we can see a non-significant ($p$ = `r round(icc_data$results$p[1], 3)`) low correlation (ICC = `r round(icc_data$results$ICC[1], 3)`).
Based on both the final omnibus test, we feel safe to proceed.

## Normality

Schumacker [@schumacker2015using] recommends testing for both univariate and multivariate normality as a reasonable practice.

```{r, results = 'hold'}
univariate <- function(data, column) {
  t1 = ad.test(data[, column])
  t2 = cvm.test(data[, column])
  t3 = lillie.test(data[, column])
  t4 = pearson.test(data[, column])
  t5 = sf.test(data[, column])
  
  t2s = function(t) {
    sprintf('%.3f (p = %.2f)',
            unname(t$statistic),
            unname(t$p.value)) }
  
  c(t2s(t1), t2s(t2), t2s(t3), t2s(t4), t2s(t5))
}

dv <- data[, !names(data) %in% c('id', 'author')]
columns <- colnames(dv)
cl <- length(columns)
tmp <- vector(mode="list", length = cl)
for(i in 1:cl) { tmp[[i]] <- univariate(dv, columns[i]) }

normality <- do.call(rbind, tmp)
rownames(normality) <- columns
colnames(normality) <- c('Anderson Darling', 'Cramer von Mises', 'Kolmogorov Smirnov', 'Pearson Chi Square', 'Shapiro Francia')
```

As we can see from Table \@ref(tab:univariate-normality), only MATTR1k _might_ be normal.
Our cadence measures consistently reject the null hypothesis of normality.
Idealy we would like all 5 tests to agree (all p > .05), but on real data, that is rare.

```{r univariate-normality, results = 'hold', echo = F}
knitr::kable(normality, booktabs = TRUE, caption = "Univariate Normality")
```

Interestingly enough, it is possible to be univariate normal, but not multivariate normal and vise versa.
As the assumption is multivariate normal, make sure that even if all univariate tests pass, you check the other.

```{r, results = 'hold'}
(st = dv %>% t() %>% shapiro.test())
```

The results from the multivariate normality test reject the null hypothesis, so we find evidence that the distribution is not jointly normal ($W$ = `r round(unname(st$statistic), 3)`, $p$ = `r round(st$p.value, 3)`).
So we need to transform the values.
Based on Table \@ref(tab:univariate-normality) we are pretty sure we need to adjust them all.

```{r, results = 'hold'}
dv[,1] <- dv[,1] %>% apply_bcskew0
dv[,2] <- dv[,2] %>% apply_bcskew0
dv[,3] <- dv[,3] %>% apply_bcskew0
dv[,4] <- dv[,4] %>% apply_bcskew0

tmp <- vector(mode="list", length = cl)
for(i in 1:cl) { tmp[[i]] <- univariate(dv, columns[i]) }
transformed_normality <- do.call(rbind, tmp)
rownames(transformed_normality) <- columns
colnames(transformed_normality) <- c('Anderson Darling', 'Cramer von Mises', 'Kolmogorov Smirnov', 'Pearson Chi Square', 'Shapiro Francia')
(tst = dv %>% t() %>% shapiro.test())
```

As we can see from Table \@ref(tab:transformed-univariate-normality), after the Box-Cox transform, all univariate tests across all measures agree on normality.
After the transform, while we still have a similar p value ($p$ = `r round(st$p.value, 3)`) from the multivariate result, the $W$ (the important part) is vastly improved ($W$ = `r round(unname(st$statistic), 3)` to $W$ = `r round(unname(tst$statistic), 3)`), though still not quite in the .90+ range we would ideally like to see.
Based on both the univariate and multivariate results, we feel safe to proceed.

```{r transformed-univariate-normality, results = 'hold', echo = F}
knitr::kable(transformed_normality, booktabs = TRUE, caption = "Univariate Normality with adjusted Box Cox skew 0")
```

# Equal variance-covariance matrix

```{r}
# iv <- data[, names(data) %in% c('id', 'author')]
# data <- cbind(iv, dv)
# bm <- boxM(data[, !names(data) %in% c('id', 'author', 'MATTR500')], data[, 'author'])
```