---
title: 'Metalinguistic Analysis'
author: "Mark Newman"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, echo = F, results = 'hide', error = F, warning = F, message = F}
rm(list = ls())
options(scipen = 999)

libraries <- c('dplyr', 'tidyr', 'ggpubr', 'mvnormtest', 'biotools', 'psych', 'heplots')
# if needed
# install.packages(libraries)
invisible(lapply(libraries, require, character.only = TRUE))

source("./utils.r")
libraries <- c(libraries, util_libraries)
```

A Metalinguistic Analysis compares the linguistic features of a corpus across some metadata segment.
The features are extracted on a document by document basis.
Then a MANOVA is preformed comparing the segments.

In order to perform a Metalinguistic Analysis, documents must first be obtained.
This can be done by any means.
Following data collection, the documents need to be processed and the features extracted.
R is _OK_ enough at this task, but as the document count increases, R _really_ slows down.
Because of this, the feature extraction step is preformed in Pure Python.
The walk-through of this can be found in `../code/01-MetalinguisticAnalysisPipeline.md`

# Data format
 
For this recipe, 3 files are expected:

* A `metadata.csv` file containing metadata data about the individual documents.
  This file needs to include at _least_ 2 columns:
  1. **id**: the document's id
  2. The segment (i.e. **author**)
* A `cadence.csv` file containing frequency count measures.
  This file will contain 4 columns:
  1. **id**: the document's id
  2. **Words**: Total words in the document
  3. **Sentences**: Total sentences in the document
  4. **Paragraphs**: Total paragraphs in the document
* A `diversity.csv` file containing diversity measures.
  This file will contain 4 columns:
  1. **id**: the document's id
  2. **TTR**: the type-token ratio (TTR)
  3. **MATTR500**: the moving average type-token ratio (MATTR)
     * Window length 500
  4. **MATTR1k**: the moving average type-token ratio
     * Window length 1000

```{r, results = 'hold'}
metadata <- read.csv('../data/metadata.csv')
cadence <- read.csv('../data/cadence.csv')
diversity <- read.csv('../data/diversity.csv')
```

# Check Assumptions.

Normally, MANOVA is known to be robust.
What this _actualy_ means is to works well if the assumptions are violated, but only under certain circumstances.
The circumstance being a balanced design.
Unfortunately, a metalinguistic analysis is almost never balanced.
This means we are at least forced to use type III SS measures

The assumptions we need to test are as follows:

* Independent Observations
* Normality
* Equal variance-covariance matrix

## Charts

Looking at charts helps inform us about our data.
They are not a _test_ of anything, but they can help us make decisions out of the gate.

```{r}
cadence_words <-
  metadata %>%
  merge(cadence) %>%
  transmute(author, Words) %>%
  gather(type, count, -author) %>%
  mutate(type = as.factor(type))

word_density_plot <-
  ggdensity(cadence_words,
    x = 'count',
    add = 'mean',
    color = 'type',
    palette = get_palette('Dark2', 1),
    title = 'Word Distribution')

cadence_sentence <-
  metadata %>%
  merge(cadence) %>%
  transmute(author, Sentences) %>%
  gather(type, count, -author) %>%
  mutate(type = as.factor(type))

sentence_density_plot <-
  ggdensity(cadence_sentence,
    x = 'count',
    add = 'mean',
    color = 'type',
    palette = get_palette('Dark2', 1),
    title = 'Sentence Distribution')

cadence_paragraph <-
  metadata %>%
  merge(cadence) %>%
  transmute(author, Paragraphs) %>%
  gather(type, count, -author) %>%
  mutate(type = as.factor(type))

paragraph_density_plot <-
  ggdensity(cadence_paragraph,
    x = 'count',
    add = 'mean',
    color = 'type',
    palette = get_palette('Dark2', 1),
    title = 'Paragraph Distribution')

diversity2 <-
  metadata %>%
  merge(diversity) %>%
  transmute(author, TTR, MATTR500, MATTR1k) %>%
  gather(type, count, -author) %>%
  mutate(type = as.factor(type))

diversity_density_plot <-
  ggdensity(diversity2,
    x = 'count',
    add = 'mean',
    color = 'type',
    palette = get_palette('Dark2', 3),
    title = 'TTR / MATTR Distribution')

```

```{r word-density}
plot(word_density_plot)
```

```{r sentence-density}
plot(sentence_density_plot)
```

```{r paragraph-density}
plot(paragraph_density_plot)
```

```{r diversity-density}
plot(diversity_density_plot)
```

All the figures look like they _might_ already be normal, or that at least we can wrangle them into being normal.

## Independent Observations

Independent observations doesn't mean that one document was accedently included in the corpus twice, but rather that two of the measures are actualy measuring the same thing.

```{r, results = 'hold'}
(icc_cadence <- ICC(cadence[, !names(cadence) %in% c('id')]))
```

Looking at the 1st ICC, we can see a non-significant (p = `r round(icc_cadence$results$p[1], 3)`) low correlation (ICC = `r round(icc_cadence$results$ICC[1], 3)`).
The omnibus test fails to reject the null hypothesis and so we conclude that there is not sufficient evidence to support the different measures being dependent.
_If_ the measures _were_ dependent we loose the rational for running a MANOVA.
A simple ANOVA on the strongest measure (usually words) would be the next step

```{r, results = 'hold'}
(icc_diversity <- ICC(diversity[, !names(cadence) %in% c('id')]))
```

Looking at the 1st ICC, we can see a non-significant (p = `r round(icc_diversity$results$p[1], 3)`) low correlation (ICC = `r round(icc_diversity$results$ICC[1], 3)`).
The omnibus test fails to reject the null hypothesis and so we conclude that there is not sufficient evidence to support the different measures being dependent.
However, we know our data.
The MATTR is essentially trying to get a better estimate of the true document normal diversity when compared to the normal TTR.
So it makes sense to check some pair wise values too.

```{r, results = 'hold'}
(ct_1 <- cor.test(diversity[, 'TTR'], diversity[, 'MATTR500']))
(ct_2 <- cor.test(diversity[, 'TTR'], diversity[, 'MATTR1k']))
(ct_3 <- cor.test(diversity[, 'MATTR500'], diversity[, 'MATTR1k']))
```

When comparing TTR to either MATTR (either 500 or 1000), we find a significant ($p$ = `r round(ct_1$p.value, 3)` / $p$ = `r round(ct_2$p.value, 3)`) medium correlation ($r$ = `r round(unname(ct_1$estimate), 3)` / $r$ = `r round(unname(ct_2$estimate), 3)`).
In both cases, the pair-wise test rejects the null hypothesis and so we concluder that there is evidence to support the different measures being dependent.
When comparing the two different window lengths of MATTR, we find a significant ($p$ = `r round(ct_3$p.value, 3)`) hi correlation ($r$ = `r round(unname(ct_3$estimate), 3)`).
The pair-wise test rejects the null hypothesis and so we concluder that there is evidence to support the different measures being dependent.
Based on the above evidence, we elect to proceed with just the MATTR1k.

Gather up all the dependent variables at interest into a single frame.
Preform one last omnibus ICC to make sure we are still good.

```{r, results = 'hold'}
data <-
  metadata %>%
  merge(cadence) %>%
  merge(diversity) %>%
  transmute(author, Words, Sentences, Paragraphs, MATTR1k)

(icc_data = ICC(data[, !names(data) %in% c('author')]))
```

Looking at the 1st ICC, we can see a non-significant ($p$ = `r round(icc_data$results$p[1], 3)`) low correlation (ICC = `r round(icc_data$results$ICC[1], 3)`).
Based on both the final omnibus test, we feel safe to proceed.

## Normality

Normality only matters for the DVs.
So split appart the IVs and DVs.

```{r, results = 'hold'}
iv_columns <- c('author')
iv <- data[, names(data) %in% iv_columns]
dv <- data[, !names(data) %in% iv_columns]
```

Even though the assumption is joint multivariate normality, Schumacker [@schumacker2015using] recommends that testing for both univariate and multivariate normality as a good practice.
Addtionaly, it is possible to be univariate normal, but not multivariate normal and vise versa.
As the assumption is multivariate normal, make sure that even if all univariate tests pass, you check the other.

```{r, results = 'hold'}
normality <- test_univariate_normality(dv)
(st <- dv %>% t() %>% shapiro.test())
```

As we can see from Table \@ref(tab:univariate-normality), only MATTR1k _might_ be normal.
Our cadence measures consistently reject the null hypothesis of normality.
Idealy we would like all 5 tests to agree (all p > .05), but on real data, that is rare.
Furtheremore, the results from the multivariate normality test reject the null hypothesis, so we find evidence that the distribution is not jointly normal ($W$ = `r round(unname(st$statistic), 3)`, $p$ `r apa_pvalue(st$p.value)`).
So we need to transform the values.
Based on Table \@ref(tab:univariate-normality) we are pretty sure we need to adjust them all.

```{r univariate-normality, results = 'hold', echo = F}
knitr::kable(normality, booktabs = TRUE, caption = "Univariate Normality")
```

```{r, results = 'hold'}
dv[,1] <- dv[,1] %>% apply_bcskew0 %>% as.vector()
dv[,2] <- dv[,2] %>% apply_bcskew0 %>% as.vector()
dv[,3] <- dv[,3] %>% apply_bcskew0 %>% as.vector()
dv[,4] <- dv[,4] %>% apply_bcskew0 %>% as.vector()

transformed_normality <- test_univariate_normality(dv)
(tst = dv %>% t() %>% shapiro.test())
```

As we can see from Table \@ref(tab:transformed-univariate-normality), after the Box-Cox transform, all univariate tests across all measures agree on normality.
After the transform, while we still have a similar p-value ($p$ `r apa_pvalue(tst$p.value)`) to the untransformed multivariate result, the $W$ (the important part) is vastly improved ($W$ = `r round(unname(st$statistic), 3)` to $W$ = `r round(unname(tst$statistic), 3)`), though still not quite in the .90+ range we would ideally like to see.
Based on both the univariate and multivariate results, there is sufficent evedence to proceed _with causion_.

```{r transformed-univariate-normality, results = 'hold', echo = F}
knitr::kable(transformed_normality, booktabs = TRUE, caption = "Univariate Normality with adjusted Box Cox skew 0")
```

Stitch back together the IVs and the the transformed DVs.

```{r, results = 'hold'}
cn <- colnames(data)
data <- cbind(iv, dv)
colnames(data) <- cn
```

## Equal variance-covariance matrix

Make sure we can even preform the analysis by testing for positive non-zero determinates.

```{r, results = 'hold'}
groups <- data$author %>% unique()
posdet <- vector(mode = 'numeric', length = length(groups))
for(i in 1:length(groups)) {
  posdet[i] <- data %>%
    filter(author == groups[i]) %>%
    .[, !names(data) %in% iv_columns] %>%
    cov() %>%
    det()
}
posdet
```

All the different slices have positive non-zero determinates.
Now check the overall covariance matrix

```{r, results = 'hold'}
(bm <- boxM(data[, !names(data) %in% iv_columns], data[, names(data) %in% iv_columns]))
```

As we can see from `boxM()`, the result is signifigant ($p$ `r apa_pvalue(bm$p.value)`) so we reject the null hypothesis and say there is evedence to support the variance-covariance matrix being un-equal.
`boxM()` is known to be particularly subsceptiable to normality violations so the not quite good enough $W$ probaly did the analysis in.
This means that we can't use a traditional `manova()` command, instead we need to use a `lm()` and convert it to a MANOVA.

# Analysis and Results

As stated above, we can't use `manova()` directly and must use Type III SS in our analysis.
Any time a `lm()` is converted into a MANOVA, the `lm()` residules need to be checked for normality in order to yield more confidence in a model fit. 

```{r, results = 'hold'}
model <- lm(cbind(Words, Sentences, Paragraphs, MATTR1k) ~ author, data = data)
(st <- model$residuals %>% t() %>% shapiro.test())
Manova(model, multivariate = T, type = c('III'), test = 'Wilks')
mv_tab <- lm_to_anova(model, multivariate = T, type = c('III'))
mv_eta <- etasq(model)[[1]]
```

As can be seen from Wilks, _author_ is jointly statisticaly significant $F$(`r mv_tab[2,4]`, `r mv_tab[2,5]`) = `r round(mv_tab[2,3], 3)` ($p$ `r apa_pvalue(mv_tab[2,6])`) with a reasonable model fit ($W$ = `r round(unname(st$statistic), 3)`, $p$ `r apa_pvalue(st$p.value)`) and an effect size of Partial $\eta^{2}$ = `r round(mv_eta, 3)`.
Table \@ref(tab:univariate-results) presents the means, standard deviations, and follow-up univariate ANOVA Bonferroni adjusted results on the 4 variables.

```{r univariate-results, results = 'hold', echo = F}
title <- 'The means, standard deviations, F values, degrees of freedom, p values, and effect sizes for the 4 variables analyzed.'

dba <- describeBy(data, group = 'author')

column_names <- c(
  sprintf('CD (N=%i) M(SD)', dba$CD$n[1]),
  sprintf('JA (N=%i) M(SD)', dba$JA$n[1]),
  sprintf('MT (N=%i) M(SD)', dba$MT$n[1]),
  'F', 'P', '$\\eta^{2}$')

m1 <- lm(Words ~ author, data = data)
m2 <- lm(Sentences ~ author, data = data)
m3 <- lm(Paragraphs ~ author, data = data)
m4 <- lm(MATTR1k ~ author, data = data)
s1 <- lm_to_anova(m1, type = c('III'))
s2 <- lm_to_anova(m2, type = c('III'))
s3 <- lm_to_anova(m3, type = c('III'))
s4 <- lm_to_anova(m4, type = c('III'))

ex <- function(model) { etasq(model)[1,1] }

univariate_results <-
cbind(
  sprintf('%.2f (%.2f)', dba$CD$mean[2:5], dba$CD$sd[2:5]),
  sprintf('%.2f (%.2f)', dba$JA$mean[2:5], dba$JA$sd[2:5]),
  sprintf('%.2f (%.2f)', dba$MT$mean[2:5], dba$MT$sd[2:5]),
  round(c(s1[2,3], s2[2,3], s3[2,3], s4[2,3]), 3),
  apa_pvalue(p.adjust(c(s1[2,4], s2[2,4], s3[2,4], s4[2,4]), method = 'bonferroni')),
  round(c(ex(m1), ex(m2), ex(m3), ex(m4)), 3)
)
colnames(univariate_results) <- column_names
rownames(univariate_results) <- rownames(dba$CD)[2:5]

knitr::kable(univariate_results, booktabs = T, caption = title)
```


